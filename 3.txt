AIM: Practical of Principal Component Analysis(PCA).
Theory: PCA is a very popular method of dimensionality reduction because it provides a way to easily reduce the dimensions and is easy to understand. For this reason, PCA has been used in various applications from image compression to complex gene comparison. While using PCA, one should keep in mind its limitations well. PCA is very sensitive to the scale of the data. It will create an initial basis in the direction of the largest variance in the data. Moreover, PCA applies a transformation over the data where all new components are orthogonal. The new features may not be interpretable in business. Another limitation of PCA is the reliance on the mean and variance of the data. If the data has a relationship in higher dimensions such as kurtosis and skewness then PCA may not be the right technique to use on the data. In situations when the features are already orthogonal to each other and are uncorrelated, PCA will not produce any useful results except ordering the features in decreasing order of their variances. PCA is very useful in situations when the data at hand is very large. Example, in case of image compression, PCA can be used to store the image in the first few hundred components and use less number of pixels.
We can implement the same in R programming language.

commands:
> data_iris <- iris[1:4]
> Cov_data <- cov(data_iris )
> Eigen_data <- eigen(Cov_data)
> PCA_data <- princomp(data_iris ,cor="False")
> Eigen_data$values
> PCA_data$sdev^2
> PCA_data$loadings[,1:4]
> Eigen_data$vectors
> summary(PCA_data)
> biplot (PCA_data)
> screeplot(PCA_data, type="lines")
> model2 = PCA_data$loadings[,1]
> model2_scores <- as.matrix(data_iris) %*% model2
> library(class) 
> install.packages("e1071") 
> library(e1071)
> mod1<-naiveBayes(iris[,1:4], iris[,5])
> mod2<-naiveBayes(model2_scores, iris[,5])
>table(predict(mod1, iris[,1:4]), iris[,5])
>table(predict(mod2, model2_scores), iris[,5])

